#+TITLE: New and modified functionality in xcms
#+AUTHOR:    Johannes Rainer
#+EMAIL:     johannes.rainer@eurac.edu
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS: ^:{} toc:nil
#+PROPERTY: exports code
#+PROPERTY: session *R*

#+BEGIN_html
---
title: "New and modified functionality in xcms"
graphics: yes
output:
  BiocStyle::html_document2:
    toc: false
vignette: >
  %\VignetteIndexEntry{New and modified functionality in xcms}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{xcms,RColorBrewer}
  %\VignettePackage{xcms}
  %\VignetteKeywords{mass spectrometry, metabolomics}
bibliography: references.bib
csl: biomed-central.csl
references:
- id: dummy
  title: no title
  author:
  - family: noname
    given: noname
---

```{r style, echo = FALSE, results = 'asis', message=FALSE}
BiocStyle::markdown()
```

#+END_html

#+BEGIN_html
**Package**: `r Biocpkg("xcms")`<br />
**Authors**: Johannes Rainer<br />
**Modified**: 5 September, 2016<br />
**Compiled**: `r date()`
#+END_html


* New functionality in =xcms=

This document describes new functionality and changes to existing functionality
in the =xcms= package introduced during the update to version /3/.

#+BEGIN_SRC R :ravel message = FALSE
  library(xcms)
  library(RColorBrewer)
#+END_SRC

** Binning and missing value imputation functions

The binning/profile matrix generation functions have been completely
rewritten. The new =binYonX= function replaces the binning of intensity values
into bins defined by their m/z values implemented in the =profBin=, =profBinLin= and
=profBinLinBase= methods. The =binYonX= function provides also additional functionality:

+ Breaks for the bins can be defined based on either the number of desired bins
  (=nBins=) or the size of a bin (=binSize=). In addition it is possible to provide
  a vector with pre-defined breaks. This allows to bin data from multiple files
  or scans on the same bin-definition.

+ The function returns a list with element =y= containing the binned values and
  element =x= the bin mid-points.

+ Values in input vector =y= can be aggregated within each bin with different
  methods: =max=, =min=, =sum= and =mean=.

+ The index of the largest (or smallest for =method= being "min") within each bin
  can be returned by setting argument =returnIndex= to =TRUE=.

+ Binning can be performed on single or multiple sub-sets of the input vectors
  using the =fromIdx= and =toIdx= arguments. This replaces the /M/ methods (such as
  =profBinM=). These sub-sets can be overlapping.

The missing value imputation logic inherently build into the =profBinLin= and
=profBinLinBase= methods has been implemented in the =imputeLinInterpol= function.

The example below illustrates the binning and imputation with the =binYtoX= and
=imputeLinInterpol= functions. After binning of the test vectors below some of the
bins have missing values, for which we impute a value using
=imputeLinInterpol=. By default, =binYonX= selects the largest value within each
bin, but other aggregation methods are also available (i.e. min, max, mean,
sum).

#+BEGIN_SRC R :ravel message = FALSE
  ## Defining the variables:
  set.seed(123)
  X <- sort(abs(rnorm(30, mean = 20, sd = 25))) ## 10
  Y <- abs(rnorm(30, mean = 50, sd = 30))

  ## Bin the values in Y into 20 bins defined on X
  res <- binYonX(X, Y, nBins = 22)

  res
#+END_SRC

As a result we get a =list= with the bin mid-points (=$x=) and the binned =y= values
(=$y=).

Next we use two different imputation approaches, a simple linear interpolation
and the linear imputation approach that was defined in the =profBinLinBase=
method. The latter performs linear interpolation only considering a certain
neighborhood of missing values otherwise replacing the =NA= with a base value.

#+BEGIN_SRC R :ravel binning-imputation-example, message = FALSE, fig.width = 10, fig.height = 7, fig.cap = 'Binning and missing value imputation results. Black points represent the input values, red the results from the binning and blue and green the results from the imputation (with method lin and linbase, respectively).'
  ## Plot the actual data values.
  plot(X, Y, pch = 16, ylim = c(0, max(Y)))
  ## Visualizing the bins
  abline(v = breaks_on_nBins(min(X), max(X), nBins = 22), col = "grey")

  ## Define colors:
  point_colors <- paste0(brewer.pal(4, "Set1"), 80)
  ## Plot the binned values.
  points(x = res$x, y = res$y, col = point_colors[1], pch = 15)

  ## Perform the linear imputation.
  res_lin <- imputeLinInterpol(res$y)

  points(x = res$x, y = res_lin, col = point_colors[2], type = "b")

  ## Perform the linear imputation "linbase"
  res_linbase <- imputeLinInterpol(res$y, method = "linbase")
  points(x = res$x, y = res_linbase, col = point_colors[3], type = "b", lty = 2)
#+END_SRC

The difference between the linear interpolation method =lin= and =linbase= is that
the latter only performs the linear interpolation in a pre-defined neighborhood
of the bin with the missing value (=1= by default). The other missing values are
set to a base value corresponding to half of the smallest bin value. Both
methods thus yield same results, except for bins 15-17 (see Figure above).


** Core feature detection functions

The core logic from the feature detection methods =findPeaks.centWave=,
=findPeaks.massifquant=, =findPeaks.matchedFilter= has been extracted and put into
functions with the common prefix =do_detectFeatures= with the aim, as detailed in
issue [[https://github.com/sneumann/xcms/issues/30][#30]], to separate the core logic from the analysis methods invoked by the
users to enable also the use of the feature detection functions using base R
parameters (i.e. without specific classes containing the data such as the
=xcmsRaw= class). This simplifies also the re-use of these functions in other
packages. The implemented functions are:

+ =do_detectFeatures_centWave=: peak density and wavelet based feature detection
  for high resolution LC/MS data in centroid mode \cite{Tautenhahn:2008fx}.
+ =do_detectFeatures_matchedFilter=: identification of features in the
  chromatographic domain based on matched filtration \cite{Smith:2006ic}.


** Usability improvements

+ =[= subsetting method for =xcmsRaw= objects that enables to subset an =xcmsRaw=
  object to specific scans/spectra.

* Changes due to bug fixes and modified functionality

** Differences in linear interpolation of missing values (=profBinLin=).

Two bugs are present in the =profBinLin= method (reported as issues [[https://github.com/sneumann/xcms/issues/46][#46]] and [[https://github.com/sneumann/xcms/issues/49][#49]] on
github) which are fixed in the new =binYonX= and =imputeLinInterpol= functions:

+ The first bin value calculated by =profBinLin= can be wrong (i.e. not being the
  max value within that bin, but the first).
+ If the last bin contains also missing values, the method fails to determine
  a correct value for that bin.

The =profBinLin= method is used in =findPeaks.matchedFilter= if the profile
method is set to "binlin".

The example below illustrates both differences.

#+BEGIN_SRC R
  ## Define a vector with empty values at the end.
  X <- 1:11
  set.seed(123)
  Y <- sort(rnorm(11, mean = 20, sd = 10))
  Y[9:11] <- NA
  nas <- is.na(Y)
  ## Do interpolation with profBinLin:
  resX <- xcms:::profBinLin(X[!nas], Y[!nas], 5, xstart = min(X),
                            xend = max(X))
  resX
  res <- binYonX(X, Y, nBins = 5L, shiftByHalfBinSize = TRUE)
  resM <- imputeLinInterpol(res$y, method = "lin",
                            noInterpolAtEnds = TRUE)
  resM
#+END_SRC

Plotting the results helps to better compare the differences. The black points
in the figure below represent the actual values of =Y= and the grey vertical lines
the breaks defining the bins. The blue lines and points represent the result
from the =profBinLin= method. The bin values for the first and 4th bin are clearly
wrong. The green colored points and lines represent the results from the =binYonX=
and =imputeLinInterpol= functions (showing the correct binning and interpolation).

#+BEGIN_SRC R :ravel profBinLin-problems, message = FALSE, fig.align = 'center', fig.width=10, fig.height = 7, fig.cap = "Illustration of the two bugs in profBinLin. The input values are represented by black points, grey vertical lines indicate the bins. The results from binning and interpolation with profBinLin are shown in blue and those from binYonX in combination with imputeLinInterpol in green."
  plot(x = X, y = Y, pch = 16, ylim = c(0, max(Y, na.rm = TRUE)),
       xlim = c(0, 12))
  ## Plot the breaks
  abline(v = breaks_on_nBins(min(X), max(X), 5L, TRUE), col = "grey")
  ## Result from profBinLin:
  points(x = res$x, y = resX, col = "blue", type = "b")
  ## Results from imputeLinInterpol
  points(x = res$x, y = resM, col = "green", type = "b",
         pch = 4, lty = 2)

#+END_SRC

Note that by default =imputeLinInterpol= would also interpolate missing values at
the beginning and the end of the provided numeric vector. This can be disabled
(to be compliant with =profBinLin=) by setting parameter =noInterpolAtEnds= to
=TRUE= (like in the example above).


** Differences due to updates in =do_detectFeatures_matchedFilter=, respectively =findPeaks.matchedFilter=.

The original =findPeaks.matchedFilter= (up to version XXX) had several
shortcomings and bugs that have been fixed in the new
=do_detectFeatures_matchedFilter= method:

+ The internal iterative processing of smaller chunks of the full data (also
  referred to as /iterative buffering/) could result, for some bin (step) sizes to
  unstable binning results (discussed in issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github): calculation of
  the breaks, or to be precise, the actually used bin size was performed in each
  iteration and could lead to slightly different sizes between iterations (due
  to rounding errors caused by floating point number representations in C).

+ The iterative buffering raises also a conceptual issue when linear
  interpolation is performed to impute missing values: the linear imputation
  will only consider values within the actually processed buffer and can thus
  lead to wrong or inaccurate imputations.

+ The =profBinLin= implementation contains two bugs, one that can result in
  failing to identify the maximal value in the first and last bin (see issue
  [[https://github.com/sneumann/xcms/issues/46][#46]]) and one that fails to assign a value to a bin (issue [[https://github.com/sneumann/xcms/issues/49][#49]]). Both are fixed
  in the =do_detectFeatures_matchedFilter= implementation.

A detailed description of tests comparing all implementations is available in
issue [[https://github.com/sneumann/xcms/issues/52][#52]] on github.

*NOTE* by default the =findPeaks.matchedFilter= and =do_detectFeatures_matchedFilter=
use the /original/ (old) code, to switch to the new implementation call
=useOriginalCode(FALSE)=.


** Differences in =findPeaks.massifquant=

+ Argument =scanrange= was ignored in the /original/ old code (issue [[https://github.com/sneumann/xcms/issues/61][#61]]).
+ The method returned a =matrix= if =withWave= was =0= and a =xcmsPeaks= object
  otherwise. The updated version returns *always* an =xcmsPeaks= object (issue #60).

** =scanrange= parameter in all =findPeaks= methods

The =scanrange= in the =findPeaks= methods is supposed to enable the peak detection
only within a user-defined range of scans. This was however not performed in
each method. Due to a bug in =findPeaks.matchedFilter='s original code the
argument was ignored, except if the upper scan number of the user defined range
was larger than the total number of available scans (see issue [[https://github.com/sneumann/xcms/issues/63][#63]]). In
=findPeaks.massifquant= the argument was completely ignored (see issue [[https://github.com/sneumann/xcms/issues/61][#61]]) and,
while the argument was considered in =findPeaks.centWave= and feature detection
was performed within the specified scan range, but the original =@scantime= slot
was used throughout the code instead of just the scan times for the specified
scan indices (see issue [[https://github.com/sneumann/xcms/issues/64][#64]]).

These problems have been fixed by first sub-setting the =xcmsRaw= object (using
the =[= method) before actually performing the feature detection.


** Problems with iterative binning of small data sub-sets in =findPeaks.matchedFilter= :noexport:

The iterative binning of only small sub-sets of data causes problems with
=profBinLinBase=, in which data imputation might be skipped in some iterations
while it is performed in others (also discussed in issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github).

Iterative buffering has both conceptual and computational issues.
+ Conceptual: =profBinLin= and =profBinLinBase= do a linear interpolation to impute
  missing values. This is obviously affected by the input data, i.e. if only a
  small subset of input data is considered, the imputation can change.

+ Computational: the iterative buffering is slower than binning of the full
  data.

An additional problem comes with the implementation of the =profBin= method in
=xcms= that was used in the =findPeaks.matchedFilter= method for method being =lin=:
the bin size is calculated anew in each call, thus, due to rounding errors
(imprecision of floating point numbers), the bin size will be slightly different
in each call, which can lead to wrong binning results (see issue [[https://github.com/sneumann/xcms/issues/47][#47]] on github).

Example with =profBinLinBase= resulting in an error: if =step= and =basespace= are
both =0.1= it seems that not in all buffer-generation iterations a interpolation
is initiated, i.e. the variable =ibase= in the C-function is sometimes set to =1=
(interpolation with neighboring bins) and sometimes to =0=.

This is also extensively documented in issue [[https://github.com/sneumann/xcms/issues/52][#52]].

** Different binning results due to /internal/ and /external/ breaks definition :noexport:

*FIXED*: the bin calculation in C uses now also a multiplication instead of a
addition thus resulting in identical breaks!

Breaks calculated by the =breaks_on_nBins= function are equal as breaks calculated
using the =seq= function, but they are not identical.

#+BEGIN_SRC R
  library(xcms)

  ## Define breaks from 200 to 600
  brks <- seq(200, 600, length.out = 2002)
  brks2 <- xcms:::breaks_on_nBins(200, 600, nBins = 2001)
  all.equal(brks, brks2)
  identical(brks, brks2)

  ## The difference is very small, but could still, in the binning
  ## yield slightly different results depending on which breaks are
  ## used.
  range(brks - brks2)
#+END_SRC

** Implementation and comparison for =matchedFilter=		   :noexport:

These results base on the test =dontrun_test_do_detectFeatures_matchedFilter_impl=
defined in /test_do_detectFeatures_matchedFilter.R/

We have 4 different functions to test and compare to the original one:
+ *A*: =.matchedFilter_orig=: it's the original code.
+ *B*: =.matchedFilter_binYonX_iter=: uses the same sequential
  buffering than the original code, but uses =binYonX= for binning and
  =imputeLinInterpol= for interpolation.
+ *C*: =.matchedFilter_no_iter=: contains the original code, but
  avoids sequential buffering, i.e. creates the whole matrix in one go.
+ *D*: =.matchedFilter_binYonX_no_iter=: my favorite: uses =binYonX= and
  =imputeLinInterpol= and avoids the sequential buffering by creating the full
  matrix in one go.

Notes: for plain =bin= we expect that results with and without iterative buffering
are identical.

*Comparisons*:
+ [X] *A* /vs/ original:
  - =bin=: always OK.
  - =binlin=: always OK.
  - =binlinbase=: always OK.
+ [X] *B* /vs/ original:
  - =bin=: OK unless =step= is =0.2=: most likely rounding problem.
  - =binlin=: only once OK. Results are not equal, but comparable.
  - =binlinbase=: similar but not equal.
+ [X] *C* /vs/ original:
  - =bin=: OK unless =step= is =0.2=:
  - =binlin=: never OK: due to interpolation on full, or subset data.
  - =binlinbase=: similar but not equal.
+ [X] *D* /vs/ original:
  - =bin=: OK unless =step= is =0.2=: most likely rounding problem.
  - =binlin=: never OK: due to interpolation on full, or subset data AND due to
    fix of the bug in =profBinLin=.
  - =binlinbase=: similar but not equal.
+ [X] *B* /vs/ *C*:
  - =bin=: always OK.
  - =binlin=: results similar but not equal; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: highly similar.
+ [X] *B* /vs/ *D*:
  - =bin=: always OK.
  - =binlin=: results similar but not equal; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: highly similar.
+ [X] *C* /vs/ *D*:
  - =bin=: always OK.
  - =binlin=: results almost identical; higher =snthresh= results in higher
    similarity.
  - =binlinbase=: always OK.


*Conclusions*:
+ =none= (only binning, but no linear interpolation; corresponds to method =bin= in
  =findPeaks.matchedFilter=): The results are identical between all methods for
  all except one setting: with =step= being =0.2= (or =0.4= etc) on one test file the
  results differ between methods with and without iterative buffering. The
  reason for this is most likely rounding errors in floating point number
  representation: =profBin= calculates the size of the bin in each call, thus,
  when called repeatedly based on different input values, the size is slightly
  different, which then can lead to binning differences (see also [[https://github.com/sneumann/xcms/issues/47][issue #47]] on
  github).

+ =lin= (binning followed by linear interpolation to impute missing values; method
  =binlin= in =findPeaks.matchedFilter=): There are two reasons for differences
  observed here: 1) the first bin value (and eventually the last bin value) are
  sometimes wrong (issue [[https://github.com/sneumann/xcms/issues/46][#46]]). This results in differences between =binYonX= and
  =imputeKinInterpol= based approach and =profBinLin= (with the former being
  presumably correct). Also, this has a bigger influence when the
  binning/missing value imputation is performed iteratively. Thus, the
  difference between the =binYonX= - =imputeLinInterpol= and =profBinLin= approach
  without iterative buffering are only very small. 2) Linear interpolation on
  the full data set compared to subsequent sub-sets will undoubtedly lead to
  differences. Because based on the full data set, the non-iterative approach
  results in the expected and more accurate results.

+ =linbase=: results are identical if =basespace= (respectively =distance=) is such
  that no interpolation takes place. With interpolation (e.g. =distance= being =1=)
  differences (albeit small) are present between approaches with and without iterative
  buffering. The results for the approaches without iterative buffering (using
  =profBinBase= respectively =binYonX= with =imputeLinIterpol=) are identical, again
  arguing in favor of these approaches.

Thus, summarizing, the approaches without the iterative buffering yield more
reliable (and presumably correct) results. Given also that the =binYonX= in
combination with =imputeLinInterpol= identify similar peaks than the non-iterative
approaches using the original code, we can change the code to use these former
methods as default.


* Introducing =DRanges=.						   :noexport:

*Note*: the code for this is in the =dranges= branch. The last status/problem is
that it is not quite clear how to determine the /correct/ number of decimal
places: =as.character= uses =options()$scipen= to determine how many decimal places
are represented, =sprintf= allows much more decimal places, e.g. with =%.30f=, but
these become unstable and random. The /best/ solution for now would be to limit to
a certain number of /secure/ decimal places (16?) and specify this as global
option that might be changed later. Check also =.Machine= for details on
precision, max integer etc. Note also that we are pretty much limited by the
largest =integer= that can be represented.

The =multiplier= thus has definitely be smaller than:
#+BEGIN_SRC R
  maxPos <- nchar(as.character(.Machine$integer.max))
  maxMult <- 10^maxPos

#+END_SRC

Note that we would actually just have to check that the to-be-transformed
integers don't get too large; thus we could allow more decimal places.

The idea is to use all of the =IRanges= functionality, but for any =numeric=
ranges. Examples for such ranges could be the m/z range of a feature, or the
retention time range defining a feature.

The idea is pretty simple, the =DRanges= (/D/ standing for /double/, alternatively /N/
for /numeric/) extends the =IRanges=, the =start= and =end= of the =IRanges= are
calculated by multiplying the start and end defining the numeric range by =10^d=
with =d= being the number of decimal places.

First thing is to get the number of decimal places: using code from a pretty old
post on stackoverflow
(http://stackoverflow.com/questions/5173692/how-to-return-number-of-decimal-places-in-r):


#+BEGIN_SRC R
  decimalplaces <- function(x) {
      if ((x %% 1) != 0) {
          nchar(strsplit(sub('0+$', '', as.character(x)), ".", fixed=TRUE)[[1]][[2]])
      } else {
          return(0)
      }
  }

  num.decimals <- function(x) {
      stopifnot(class(x)=="numeric")
      x <- sub("0+$","",x)
      x <- sub("^.+[.]","",x)
      nchar(x)
  }


#+END_SRC

The former is actually faster.

Eventually even =C=?
http://stackoverflow.com/questions/1083304/c-c-counting-the-number-of-decimals

#+BEGIN_EXAMPLE
  string number = "543.014";
  size_t dotFound;
  stoi(number, &dotFound));
  string(number).substr(dotFound).size()
#+END_EXAMPLE

Be aware that =number= MUST be a float/double!

alternatively:
http://stackoverflow.com/questions/9843999/calculate-number-of-decimal-places-for-a-float-value-without-libraries.


* Currently internal functionality				   :noexport:

** =ProcessHistory=: track processing steps

This functionality comprises the =ProcessHistory= class and the =.processHistory=
slot of the =xcmsSet= objects. The =xcmsSet= function already adds a feature
detection processing step for each file to this slot. Subsetting of =xcmsSet=
objects with =[= or =split= correctly process also this slot as does concatenation
using =c=. For processing steps other than /feature detection/ a new element should
be added to the variable =.PROCSTEPS= (defined in /DataClasses.R/.
At some point we could implement methods =getProcessErrors= and =getProcessHistory=
(essentially just calling the =.getProcessErrors= and =.getProcessHistory=
functions in /functions-xcmsSet.R/.

Some additional functionality that could be implemented:
+ Sort the processing history by the =date= slot.
+ Save also analysis properties into an object extending the =ProcessHistory=:
  this would enable to get the exact settings for each processing step.

* Internal changes						   :noexport:

** Changing the way how data is imported

Random errors happen when processing a large number of files with =xcms=. This
might indicate some memory problems, eventually related to the =mzR= package
(similar to the ones spotted in =MSnbase=).

What I want to test:
+ [X] Does =mzR::openMSFile= work also for /netCDF/? No. we would have to check for
  the file type and specify the =backend= based on that.
+ [X] What about writing a new importer that does not need all the objects and
  the presumably old code in =mzR=? -> =readRawData=.

** Functions and methods to be deprecated and removed.

+ [ ] =xcmsSource= method: not needed anymore, reading is done by =readRawData=.
+ [ ] =loadRaw=, =initialize= for =netCdfSource= and =rampSource=: replaced by
  =readRawData=.
+ [ ] =netCdfSource= and =rampSource= S4 classes: not needed anymore, reading is
  done by =readRawData=.

** Unneeded /R/ files

+ [ ] /netCDF.R/.
+ [ ] /ramp.R/.

*** Unit tests to be removed

+ [ ] /runit.ramp.R/.

* References
